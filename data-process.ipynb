{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "from pprint import pprint\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from scipy import ndimage\n",
    "import cv2\n",
    "from skimage import measure, segmentation, morphology\n",
    "from microfilm.microplot import microshow\n",
    "import hpacellseg.cellsegmentator as cellsegmentator\n",
    "from hpacellseg.utils import label_cell, label_nuclei"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook walks you through five steps to clean your dataset. Previews of the changes should be made on a \"dev dataset\" which can be a manual copy-paste of some images in a file structure similar to that of the actual dataset. Statistics are collected on these to help guide the desired parameters for later cleaning steps. Once you've set them based on the dev dataset, they will be used to process the full dataset in the background. Because of this, **please make sure your dev dataset is a good representative sample of the actual DATA**. These steps are listed below:\n",
    "1. Dataset setup: set the datasets' paths, the folder depth at which to aggregate the images into a common tensor for use at training/inference time, and the filenames for each channel and what their respective positions should be in the processed data.\n",
    "2. Cell segmentation: no parameters here at the moment, feel free to run on the full dataset once you've seen the preview on the dev dataset.\n",
    "3. Mask post-processing: removing cells that were cut off by the image edge and merging cells that were incorrectly split based on dialating their nuclei.\n",
    "4. Filtering cells based on size: delete small cells and crop the remaining images based on the size distribution of the cell bounding boxes.\n",
    "5. Normalization: based on a variety of possible choices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path(\"/data/ishang/CCNB1-dataset/\") # NEEDS TO BE ABSOLUTE PATH\n",
    "DEV_DATA_DIR = Path(\"/home/ishang/HPA-embedding/dev-dataset\") # NEEDS TO BE ABSOLUTE PATH\n",
    "# TODO this should work even if they are not pngs...\n",
    "CHANNEL_NAMES = [\"cyclinb1\", \"microtubule\", \"nuclei\"] # these are the names of the channels in the order they appear in the image, the names should be the same as the file name (without the .png extension)\n",
    "# the below are channels whose indices we need to know explicitly\n",
    "DAPI = 2\n",
    "TUBL = 1\n",
    "ANLN = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_STR, DATA_FOLDER_PATH  = str(DATA_DIR), \"data-folder.txt\"\n",
    "DEV_DATA_STR, DEV_DATA_FOLDER_PATH  = str(DEV_DATA_DIR), \"dev-data-folder.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first number in the output is the total number of images found and the second is the list of folders underwhich all images will be aggregated in the dev dataset\n",
    "# in this case we automatically aggregate everything at the first level of depth in the dataset, if you want to group by well, experiment, etc. please add the lists of absolute paths to the \"*data-folder.txt\" files accordingly\n",
    "!find $DATA_STR -type d > $DATA_FOLDER_PATH\n",
    "!sed -i '1d' $DATA_FOLDER_PATH\n",
    "!cat $DATA_FOLDER_PATH | wc -l\n",
    "!find $DEV_DATA_STR -type d > $DEV_DATA_FOLDER_PATH\n",
    "!sed -i '1d' $DEV_DATA_FOLDER_PATH\n",
    "!cat $DEV_DATA_FOLDER_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# below is an example of the level of the file hierarchy at which the data will be aggregated for the dev-dataset\n",
    "def image_paths_from_folders(folders_file):\n",
    "    image_paths = list(open(folders_file, \"r\"))\n",
    "    image_paths = [Path(x.strip()) for x in image_paths]\n",
    "    return image_paths\n",
    "image_paths = image_paths_from_folders(DATA_FOLDER_PATH)\n",
    "dev_image_paths = image_paths_from_folders(DEV_DATA_FOLDER_PATH)\n",
    "pprint(dev_image_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DO NOT RERUN THIS CELL BELOW!!! YOU WILL HAVE TO KILL THE PROCESS AND RESTART THE KERNEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_num = 7\n",
    "device = f\"cuda:{gpu_num}\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Running on device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that the source change warning below is expected\n",
    "pwd = Path(os.getcwd())\n",
    "NUC_MODEL = pwd / \"HPA-Cell-Segmentation\" / \"nuclei-model.pth\"\n",
    "CELL_MODEL = pwd / \"HPA-Cell-Segmentation\" / \"cell-model.pth\"\n",
    "\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512\"\n",
    "segmentator = cellsegmentator.CellSegmentator(\n",
    "    str(NUC_MODEL), str(CELL_MODEL), device=device, padding=True, multi_channel_model=(ANLN is not None)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DO NOT RERUN THIS ^^^^^ YOU WILL HAVE TO KILL THE PROCESS AND RESTART THE KERNEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_missing(targets, elements):\n",
    "    incomplete_elements = np.unique(elements[~np.isin(elements, targets)]) # these are cell masks\n",
    "    incomplete_elements = incomplete_elements[incomplete_elements != 0]\n",
    "    target_objects = np.stack([targets == nucleus for nucleus in np.unique(targets)])\n",
    "    target_coms = np.array([ndimage.center_of_mass(nucleus_mask) for nucleus_mask in target_objects])\n",
    "    for element in incomplete_elements:\n",
    "        # get the closest nucleus\n",
    "        element_com = ndimage.center_of_mass(elements == element)\n",
    "        target_distances = np.linalg.norm(target_coms - element_com, axis=1)\n",
    "        closest_target = np.unique(targets[target_distances == np.min(target_distances)])[0]\n",
    "        # replace the missing nucleus with the closest nucleus\n",
    "        elements[elements == element] = closest_target\n",
    "    return elements\n",
    "\n",
    "def get_masks(segmentator, image_paths, merge_missing=True):\n",
    "    \"\"\"\n",
    "    Get the masks for the images in image_paths using HPA-Cell-Segmentation\n",
    "    It places all the cells in the target directory from image_paths into a single\n",
    "    \"\"\"\n",
    "    images_paths = []\n",
    "    nuclei_mask_paths = []\n",
    "    cell_mask_paths = []\n",
    "    for image_path in image_paths:\n",
    "        if (image_path / \"images.npy\").exists():\n",
    "            images_paths.append(image_path / \"images.npy\")\n",
    "        if ((image_path / \"cell_masks.npy\").exists() and\n",
    "            (image_path / \"nuclei_masks.npy\").exists()):\n",
    "            nuclei_mask_paths.append(image_path / \"nuclei_masks.npy\")\n",
    "            cell_mask_paths.append(image_path / \"cell_masks.npy\")\n",
    "            continue\n",
    "        glob_channel_images = lambda image_path, c: list(glob(f\"{str(image_path)}/**/*{CHANNEL_NAMES[c]}.png\", recursive=True))\n",
    "        dapi_paths = sorted(glob_channel_images(image_path, DAPI))\n",
    "        tubl_paths = sorted(glob_channel_images(image_path, TUBL))\n",
    "        anln_paths = sorted(glob_channel_images(image_path, ANLN)) if ANLN is not None else None\n",
    "        \n",
    "        for dapi, tubl in zip(dapi_paths, tubl_paths):\n",
    "            assert str(dapi).split(CHANNEL_NAMES[DAPI])[0] == str(tubl).split(CHANNEL_NAMES[TUBL])[0], f\"File mismatch for {dapi} and {tubl}\"\n",
    "        if ANLN is not None and anln_paths is not None:\n",
    "            for dapi, anln in zip(dapi_paths, anln_paths):\n",
    "                assert str(dapi).split(CHANNEL_NAMES[DAPI])[0] == str(anln).split(CHANNEL_NAMES[ANLN])[0], f\"File mismatch for {dapi} and {anln}\"\n",
    "\n",
    "        load_image = lambda path_list: [cv2.imread(str(x), cv2.IMREAD_UNCHANGED) for x in path_list]\n",
    "        dapi_images = load_image(dapi_paths)\n",
    "        tubl_images = load_image(tubl_paths)\n",
    "        anln_images = load_image(anln_paths) if anln_paths is not None else None\n",
    "\n",
    "        ref_images = [tubl_images, anln_images, dapi_images]\n",
    "        nuc_segmentation = segmentator.pred_nuclei(ref_images[2])\n",
    "        cell_segmentation = segmentator.pred_cells(ref_images)\n",
    "\n",
    "        # post-processing\n",
    "        nuclei_masks, cell_masks = [], []\n",
    "        for i in range(len(ref_images[2])): # 2 because DAPI will always be present and we set the order manually\n",
    "            nuclei_mask = label_nuclei(nuc_segmentation[i])\n",
    "            nuclei_mask, cell_mask = label_cell(\n",
    "                nuc_segmentation[0], cell_segmentation[i]\n",
    "            )\n",
    "            nuclei_masks.append(nuclei_mask)\n",
    "            cell_masks.append(cell_mask)\n",
    "        nuclei_masks = np.stack(nuclei_masks, axis=0)\n",
    "        cell_masks = np.stack(cell_masks, axis=0)\n",
    "\n",
    "        # apply preprocessing mask if the user want to merge nuclei\n",
    "        # in preprocess_masks get_single_cell_mask\n",
    "        # TODO get COM for nuclei and cells, and for either without a counterpart add it to the id of the closest COM of the other type\n",
    "        # TODO how are multi-nucleated cells handled?\n",
    "        images = []\n",
    "        for c, channel in enumerate(CHANNEL_NAMES):\n",
    "            channel_paths = sorted(glob_channel_images(image_path, c))\n",
    "            channel_images = load_image(channel_paths)\n",
    "            channel_images = np.stack(channel_images, axis=0)\n",
    "            images.append(channel_images)\n",
    "        images = np.stack(images, axis=1)\n",
    "        for i in [0, -2, -1]:\n",
    "            assert images.shape[i] == nuclei_masks.shape[i] == cell_masks.shape[i], f\"Shape mismatch for images and masks in {image_path}, at index {i}, images has shape {images.shape}, nuclei_masks has shape {nuclei_masks.shape}, cell_masks has shape {cell_masks.shape}\"\n",
    "        image_idx = 0\n",
    "        for image, nuclei_mask, cell_mask in zip(images, nuclei_masks, cell_masks):\n",
    "            if set(np.unique(nuclei_mask)) != set(np.unique(cell_mask)): \n",
    "                print(f\"Mask mismatch for {image_path}, nuclei: {np.unique(nuclei_masks)}, cell: {np.unique(cell_masks)}\")\n",
    "                microshow(image[(DAPI, TUBL),], cmaps=[\"pure_blue\", \"pure_red\"], label_text=f\"Image: {Path(image_path).name}[{image_idx}] \")\n",
    "                missing_nuclei = np.asarray(list(set(np.unique(cell_mask)) - set(np.unique(nuclei_mask))))\n",
    "                if len(missing_nuclei) > 0:\n",
    "                    microshow(image[TUBL] * np.isin(cell_mask, missing_nuclei), cmaps=[\"pure_red\"], label_text=f\"Cells missing nuclei: {missing_nuclei}\")\n",
    "                missing_cells = np.asarray(list(set(np.unique(nuclei_mask)) - set(np.unique(cell_mask))))\n",
    "                if len(missing_cells) > 0:\n",
    "                    microshow(image[DAPI] * np.isin(nuclei_mask, missing_cells), cmaps=[\"pure_blue\"], label_text=f\"Nuclei without cells: {missing_cells}\")\n",
    "                microshow(cell_mask, label_text=f\"Cell mask: {np.unique(cell_mask)}\")\n",
    "                if len(missing_nuclei) > 0 and merge_missing:\n",
    "                    cell_mask = merge_missing(nuclei_mask, cell_mask)\n",
    "                    microshow(cell_mask, label_text=f\"Cell mask after merging: {np.unique(cell_mask)}\")\n",
    "                microshow(nuclei_mask, label_text=f\"Nuclei mask: {np.unique(nuclei_mask)}\")\n",
    "                if len(missing_cells) > 0 and merge_missing:\n",
    "                    nuclei_mask = merge_missing(cell_mask, nuclei_mask)\n",
    "                    microshow(nuclei_mask, label_text=f\"Nuclei mask after merging: {np.unique(nuclei_mask)}\")\n",
    "        assert np.max(nuclei_masks) > 0 and np.max(cell_masks) > 0, f\"No nuclei or cell mask found for {image_path}\"\n",
    "        np.save(image_path / \"images.npy\", images)\n",
    "        np.save(image_path / \"nuclei_masks.npy\", nuclei_masks)\n",
    "        np.save(image_path / \"cell_masks.npy\", cell_masks)\n",
    "        images_paths.append(image_path / \"images.npy\")\n",
    "        nuclei_mask_paths.append(image_path / \"nuclei_masks.npy\")\n",
    "        cell_mask_paths.append(image_path / \"cell_masks.npy\")\n",
    "    return image_paths, nuclei_mask_paths, cell_mask_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_segmentations(image_paths): \n",
    "    for image_path in image_paths:\n",
    "            if (image_path / \"images.npy\").exists():\n",
    "                os.remove(image_path / \"images.npy\") \n",
    "            if (image_path / \"cell_masks.npy\").exists():\n",
    "                os.remove(image_path / \"cell_masks.npy\")\n",
    "            if (image_path / \"nuclei_masks.npy\").exists():\n",
    "                os.remove(image_path / \"nuclei_masks.npy\")\n",
    "\n",
    "clean_segmentations(dev_image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the dev-dataset directory before generating the masks and after\n",
    "!tree $DEV_DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths, nuclei_mask_paths, cell_mask_paths = get_masks(segmentator, dev_image_paths)\n",
    "print(nuclei_mask_paths, cell_mask_paths)\n",
    "!tree $DEV_DATA_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for nuclei_path, cell_path, image_path in zip(nuclei_mask_paths, cell_mask_paths, image_paths):\n",
    "    sample_nuclei_mask = np.load(nuclei_path)\n",
    "    sample_nuclei = cv2.imread(str(image_path / (CHANNEL_NAMES[DAPI] + \".png\")), cv2.IMREAD_UNCHANGED)\n",
    "    microshow(sample_nuclei_mask)\n",
    "    microshow(sample_nuclei)\n",
    "\n",
    "    sample_cell_mask = np.load(cell_path)\n",
    "    sample_microtuble = cv2.imread(str(image_path / (CHANNEL_NAMES[TUBL] + \".png\")), cv2.IMREAD_UNCHANGED)\n",
    "    microshow(sample_cell_mask)\n",
    "    microshow(sample_microtuble)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_nuclei(nuclei_mask, cell_mask, dialation_radius=20):\n",
    "    bin_nuc_mask = (nuclei_mask > 0).astype(np.int8)\n",
    "    cls_nuc = morphology.closing(bin_nuc_mask, morphology.disk(dialation_radius))\n",
    "    # get the labels of touching nuclei\n",
    "    new_label_map = morphology.label(cls_nuc)\n",
    "    new_label_idx = np.unique(new_label_map)[1:]\n",
    "\n",
    "    new_cell_mask = np.zeros_like(cell_mask)\n",
    "    new_nuc_mask = np.zeros_like(nuclei_mask)\n",
    "    for new_label in new_label_idx:\n",
    "        # get the label of the touching nuclei\n",
    "        old_labels = np.unique(nuclei_mask[new_label_map == new_label])\n",
    "        old_labels = old_labels[old_labels != 0]\n",
    "\n",
    "        new_nuc_mask[np.isin(nuclei_mask, old_labels)] = new_label\n",
    "        new_cell_mask[np.isin(cell_mask, old_labels)] = new_label\n",
    "\n",
    "        # for old_label in old_labels:\n",
    "        #     new_cell_mask[cell_mask == old_label] = new_label\n",
    "        #     new_nuc_mask[nuclei_mask == old_label] = new_label\n",
    "    return new_nuc_mask, new_cell_mask\n",
    "\n",
    "\n",
    "def clean_cell_masks(\n",
    "    cell_mask,\n",
    "    nuclei_mask,\n",
    "    rm_border=True, # removes cells touching the border\n",
    "    remove_size=1000, # remove cells smaller than remove_size, based on the area of the bounding box, honestly could be higher, mb 2500. Make 0 to turn off.\n",
    "    dialation_radius=20, # this is for 2048x2048 images adjust as needed. Make 0 to turn off.\n",
    "    # final_size=None, # resize the mask to final_size\n",
    "):\n",
    "    num_removed = 0\n",
    "    if rm_border:\n",
    "        cleared_nuclei_mask = segmentation.clear_border(nuclei_mask)\n",
    "        keep_value = np.unique(cleared_nuclei_mask)\n",
    "        bordering_cells = np.array([[x_ in keep_value for x_ in x] for x in cell_mask]).astype(\"uint8\")\n",
    "        cleared_cell_mask = cell_mask * bordering_cells\n",
    "        num_removed = len(np.unique(nuclei_mask)) - len(keep_value)  # -1 because 0 is not a cell\n",
    "        if num_removed == np.max(nuclei_mask):\n",
    "            assert np.max(keep_value) == 0, f\"Something went wrong with clearing the border, num_removed is the same as the highest index mask in nuclei mask, but the keep_value {np.max(keep_value)} != 0\"\n",
    "        nuclei_mask = cleared_nuclei_mask\n",
    "        cell_mask = cleared_cell_mask\n",
    "\n",
    "    assert set(np.unique(nuclei_mask)) == set(np.unique(cell_mask))\n",
    "\n",
    "    # needs to be after clear border otherwise you get a boundary of nuclei that are still touching the edge\n",
    "    # maybe that is due to the interpolation method\n",
    "    # ideally this happens outside\n",
    "    # if final_size is not None:\n",
    "    #     nuclei_mask = cv2.resize(nuclei_mask, final_size, interpolation=cv2.INTER_NEAREST)\n",
    "    #     cell_mask = cv2.resize(cell_mask, final_size, interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "    ### see if nuclei are touching and merge them\n",
    "    if dialation_radius > 0:\n",
    "        nuclei_mask, cell_mask = merge_nuclei(nuclei_mask, cell_mask, dialation_radius=dialation_radius)\n",
    "        assert set(np.unique(nuclei_mask)) == set(np.unique(cell_mask))\n",
    "    else:\n",
    "        cell_mask = cell_mask\n",
    "        nuclei_mask = nuclei_mask\n",
    "\n",
    "    region_props = measure.regionprops(cell_mask, (cell_mask > 0).astype(np.uint8))\n",
    "    pre_size = len(region_props)\n",
    "    if remove_size > 0:\n",
    "        region_props = [x for x in region_props if x.area > remove_size]\n",
    "        num_removed += pre_size - len(region_props)\n",
    "    bbox_array = np.array([x.bbox for x in region_props])\n",
    "\n",
    "    # convert x1,y1,x2,y2 to x,y,w,h\n",
    "    bbox_array[:, 2] = bbox_array[:, 2] - bbox_array[:, 0]\n",
    "    bbox_array[:, 3] = bbox_array[:, 3] - bbox_array[:, 1]\n",
    "\n",
    "    # com_array = np.array([x.weighted_centroid for x in region_props])\n",
    "\n",
    "    return cell_mask, nuclei_mask, bbox_array, num_removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_num_removed = 0\n",
    "total_num_original = 0\n",
    "for cell_mask_path, nuclei_mask_path, image_path in zip(cell_mask_paths, nuclei_mask_paths, viz_image_paths):\n",
    "    elements = np.load(cell_mask_path)\n",
    "    targets = np.load(nuclei_mask_path)\n",
    "    microplot.microshow(targets)\n",
    "    microplot.microshow(elements)\n",
    "    # cell_mask, nuclei_mask, _, n_removed, n_original = save_single_cell_image(cell_mask, nuclei_mask)\n",
    "    n_original = len(np.unique(targets))\n",
    "    elements, targets, bbox_array, n_removed = clean_cell_masks(\n",
    "        elements, targets, rm_border=True, remove_size=2500, dialation_radius=0\n",
    "    )\n",
    "    total_num_removed += n_removed\n",
    "    total_num_original += n_original\n",
    "    microplot.microshow(targets)\n",
    "    microplot.microshow(elements)\n",
    "    cell_image = np.asarray(cv2.imread(str(image_path / (CHANNEL_NAMES[TUBL] + \".png\")), cv2.IMREAD_UNCHANGED))\n",
    "    nuclei_image = np.asarray(cv2.imread(str(image_path / (CHANNEL_NAMES[DAPI] + \".png\")), cv2.IMREAD_UNCHANGED))\n",
    "    image = np.stack([cell_image, nuclei_image])\n",
    "    microplot.microshow(image, cmaps=[\"pure_red\", \"pure_blue\"])\n",
    "    microplot.microshow(image * (elements > 0)[None, ...], cmaps=[\"pure_red\", \"pure_blue\"])\n",
    "print(\"fraction removed:\", total_num_removed / total_num_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_single_cell_image(\n",
    "    cell_mask,\n",
    "    nuclei_mask,\n",
    "    rm_border=True, # removes cells with nuclei touching the border\n",
    "    remove_size=2500, # remove cells smaller than remove_size, based on the area of the bounding box, honestly could be higher, mb 2500. Make 0 to turn off.\n",
    "    dialation_radius=0, # this is for 2048x2048 images adjust as needed. Make 0 to turn off.\n",
    "):\n",
    "    num_original = len(np.unique(nuclei_mask))\n",
    "    new_cell_mask, new_nuc_mask, bbox_array, num_removed = clean_cell_masks(\n",
    "        cell_mask,\n",
    "        nuclei_mask,\n",
    "        rm_border=rm_border,\n",
    "        remove_size=remove_size,\n",
    "        dialation_radius=dialation_radius,\n",
    "    )\n",
    "    return new_cell_mask, new_nuc_mask, bbox_array, num_removed, num_original\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-prep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
